# This working directory will need to be set to where you have the data files.

setwd("/Users/mmgdepartment/Dropbox/Avida-ED/Publications in progress/ALife 2016 (MW lead)")

library(psych)
library(Hmisc)

# These data files are the scored responses.  Rater 1 is the consensus human score;
# rater 2 is EvoGrader's score.


# I'm checking to see if there were any transcription errors.  Easier to just remake
# the files and rerun the analyses.

data.ABR <- read.table(file="ABR.Redone.csv", sep=",", header=T)
data.ABR.Rearranged <- read.table(file="ABR.Redone.Rearranged.csv", sep=",", header=T)

data.Mushroom <- read.table(file="Mushroom.Redone.csv", sep=",", header=T)
data.Mushroom.Rearranged <- read.table(file="Mushroom.Redone.Rearranged.csv", sep=",", header=T)

#data.ABR <- read.table(file="LB145F14ABR.csv", sep=",", header=T)
#data.ABR.Rearranged <- read.table(file="LB145F14ABRRearranged.csv", sep=",", header=T)

#data.Mushroom <- read.table(file="LB145F14Mushroom.csv", sep=",", header=T)
#data.Mushroom.Rearranged <- read.table(file="LB145F14MushroomRearranged.csv", sep=",", header=T)


# Columns 12 and 23 are Key Concept totals; 13 and 24 Naive Ideas.

number.ABR.NI <- cbind(as.character(data.ABR[,1]), as.numeric(as.character(data.ABR[,13])), as.numeric(as.character(data.ABR[,24])), as.character(data.ABR[,2]))
number.ABR.KC <- cbind(as.character(data.ABR[,1]), as.numeric(as.character(data.ABR[,12])), as.numeric(as.character(data.ABR[,23])), as.character(data.ABR[,2]))

number.Mushroom.NI <- cbind(as.character(data.Mushroom[,1]), as.numeric(as.character(data.Mushroom[,13])), as.numeric(as.character(data.Mushroom[,24])), as.character(data.Mushroom[,2]))
number.Mushroom.KC <- cbind(as.character(data.Mushroom[,1]), as.numeric(as.character(data.Mushroom[,12])), as.numeric(as.character(data.Mushroom[,23])), as.character(data.Mushroom[,2]))


#cohen.kappa(ABR.KC[,2:3])
#cohen.kappa(ABR.NI[,2:3])

cohen.kappa(data.ABR.Rearranged[,5:6])
cohen.kappa(data.Mushroom.Rearranged[,5:6])

# ABR: 0.63 (0.56 - 0.7)
# Mushroom: 0.55 (0.47 - 0.63)

# What if we subset these to look at just a) Key concepts, and b) Naive ideas?

ABR.NI <- subset(data.ABR.Rearranged, Type=="Naive")
ABR.KC <- subset(data.ABR.Rearranged, Type=="KC")
Mushroom.NI <- subset(data.Mushroom.Rearranged, Type=="NI")
Mushroom.KC <- subset(data.Mushroom.Rearranged, Type=="KC")

cohen.kappa(ABR.KC[,5:6])
cohen.kappa(ABR.NI[,5:6])

# ABR KC: 0.63 (0.56 - 0.71)
# ABR NI: 0.17 (-0.065 - 0.4)

cohen.kappa(Mushroom.KC[,5:6])
cohen.kappa(Mushroom.NI[,5:6])

# Mushroom KC: 0.51 (0.43 - 0.60)
# Mushroom NI: 0.55 (0.32 - 0.77)


# Now we want to graph these.  First, figure 1: Cohen's kappa for the two full data sets.

Full.set <- matrix(nrow=3, ncol=2)

Full.set[1, 1] <- 0.63
Full.set[2, 1] <- 0.56
Full.set[3, 1] <- 0.7
Full.set[1, 2] <- 0.55
Full.set[2, 2] <- 0.47
Full.set[3, 2] <- 0.63

rownames(Full.set) <- c("Point", "Lower", "Upper")
colnames(Full.set) <- c("Antibiotic", "Mushroom")

Divided.sets <- matrix(nrow=3, ncol=4)

Divided.sets[1, 1] <- 0.63
Divided.sets[2, 1] <- 0.56
Divided.sets[3, 1] <- 0.71
Divided.sets[1, 2] <- 0.17
Divided.sets[2, 2] <- -0.065
Divided.sets[3, 2] <- 0.40
Divided.sets[1, 3] <- 0.51
Divided.sets[2, 3] <- 0.43
Divided.sets[3, 3] <- 0.60
Divided.sets[1, 4] <- 0.55
Divided.sets[2, 4] <- 0.32
Divided.sets[3, 4] <- 0.77

#colnames(Divided.sets) <- c("Antibiotic KC", "Antibiotic NI", "Mushroom KC", "Mushroom NI")
rownames(Divided.sets) <- c("Point", "Lower", "Upper")


graphics.off()
png(file="Figure.1.png", width=6.9, height=6.9, units="in", res=300)
par(mfrow=c(1,1), oma=c(3, 3, 3, 3), bty="n", xpd=NA, mar=c(3, 3, 1, 1), font.axis = 2)


full.names <- c("Antibiotic", "Mushroom")
#barplot(Full.set[1,], beside=T, legend.text=T, col=c("lightskyblue", "darkred"), xlim=c(0, 2.5), ylim=c(0, 0.75), ylab="", xlab="", axes=F, names.arg=full.names, args.legend=list(x=1.6, y=0.75, bty="y"))

barplot(Full.set[1,], beside=T, legend.text=F, col=c("lightskyblue", "darkred"), xlim=c(0, 2.5), ylim=c(0, 0.75), ylab="", xlab="", axes=F)

segments(0.72, Full.set[2, 1], 0.72, Full.set[3, 1], lwd=2)
segments(1.9, Full.set[2, 2], 1.9, Full.set[3, 2], lwd=2)

axis(2, at=c(0, 0.70), labels=c("", ""), lwd.ticks=0, pos=0)
axis(1, at=c(0, 2.5), labels=c("", ""), lwd.ticks=0, pos=0)
axis(2, at=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), labels=c("0.0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7"), las=2, pos=0)

mtext("Question", side=1, outer=TRUE, cex=1.2, font = 2)
mtext("Inter-Rater Reliability", side=2, outer=TRUE, cex=1.2, font = 2)

dev.off()



graphics.off()
png(file="Figure.2.png", width=6.9, height=4.6, units="in", res=300)
par(mfrow=c(1,1), oma=c(3, 3, 3, 3), bty="n", xpd=NA, mar=c(3, 3, 1, 1), font.axis = 2)


Divided.names <- c("", "", "", "")


barplot(Divided.sets[1,], beside=T, col=c("lightskyblue", "mediumblue", "lightpink2", "darkred"), xlim=c(0, 5), ylim=c(-0.2, 0.75), ylab="", xlab="", axes=F)

segments(.7, Divided.sets[2, 1], .7, Divided.sets[3, 1], lwd=2)
segments(1.9, Divided.sets[2, 2], 1.9, Divided.sets[3, 2], lwd=2)
segments(3.1, Divided.sets[2, 3], 3.1, Divided.sets[3, 3], lwd=2)
segments(4.3, Divided.sets[2, 4], 4.3, Divided.sets[3, 4], lwd=2)

axis(2, at=c(0, 0.70), labels=c("", ""), lwd.ticks=0, pos=0)
axis(1, at=c(-0.2, 5), labels=c("", ""), lwd.ticks=0, pos=0)
axis(1, at=c(0.7, 1.9, 3.1, 4.3), labels=c("", "", "", ""), las=0, pos=0)
axis(1, at=c(0.7, 1.9, 3.1, 4.3), labels=c("\nAntibiotic \n KC", "\nAntibiotic \n NI", "\nMushroom \n KC", "\nMushroom \n NI"), las=0, pos=-0.1, lwd=0)
axis(2, at=c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7), labels=c("0.0", "0.1", "0.2", "0.3", "0.4", "0.5", "0.6", "0.7"), las=2, pos=0)

mtext("Question", side=1, outer=TRUE, cex=1.2, font = 2)
mtext("Inter-Rater Reliability", side=2, outer=TRUE, cex=1.2, font = 2)

dev.off()

# Next we check whether there are systematic differences in the number of key concepts or naive
# ideas that EvoGrader and our human scoring picked up:


# For some reason it's stopped treating the continuous variables as continuous.  Let's try forcing it.

number.ABR.KC[,2] <- as.numeric(as.character(number.ABR.KC[,2]))
number.ABR.NI[,2] <- as.numeric(as.character(number.ABR.NI[,2]))
number.Mushroom.KC[,2] <- as.numeric(as.character(number.Mushroom.KC[,2]))
number.Mushroom.NI[,2] <- as.numeric(as.character(number.Mushroom.NI[,2]))

number.ABR.KC[,3] <- as.numeric(as.character(number.ABR.KC[,3]))
number.ABR.NI[,3] <- as.numeric(as.character(number.ABR.NI[,3]))
number.Mushroom.KC[,3] <- as.numeric(as.character(number.Mushroom.KC[,3]))
number.Mushroom.NI[,3] <- as.numeric(as.character(number.Mushroom.NI[,3]))

summary(number.ABR.KC)
summary(number.ABR.NI)
summary(number.Mushroom.KC)
summary(number.Mushroom.NI)

# No, that's still not treating them as numbers.  These next lines will thus return errors.

t.test(number.ABR.KC[,2], number.ABR.KC[,3], paired=T, alternative=c("two.sided"))
t.test(number.ABR.NI[,2], number.ABR.NI[,3], paired=T, alternative=c("two.sided"))
t.test(number.Mushroom.KC[,2], number.Mushroom.KC[,3], paired=T, alternative=c("two.sided"))
t.test(number.Mushroom.NI[,2], number.Mushroom.NI[,3], paired=T, alternative=c("two.sided"))


# Let's try forcing them as numbers directly in the t test call.

test.1 <- t.test(as.numeric(as.character(number.ABR.KC[,2])), as.numeric(as.character(number.ABR.KC[,3])), paired=T, alternative=c("two.sided"))
test.2 <- t.test(as.numeric(as.character(number.ABR.NI[,2])), as.numeric(as.character(number.ABR.NI[,3])), paired=T, alternative=c("two.sided"))
test.3 <- t.test(as.numeric(as.character(number.Mushroom.KC[,2])), as.numeric(as.character(number.Mushroom.KC[,3])), paired=T, alternative=c("two.sided"))
test.4 <- t.test(as.numeric(as.character(number.Mushroom.NI[,2])), as.numeric(as.character(number.Mushroom.NI[,3])), paired=T, alternative=c("two.sided"))

# That worked.  Now let's get the adjusted p values out of it.

test.1[[3]]*4
test.2[[3]]*4
test.3[[3]]*4
test.4[[3]]*4


# Yes.  All 4 t tests are highly significant, even correcting for multiple tests.
# Interestingly, the lowest IRR is for the antibiotic resistance question, though there is a
# larger absolute difference in the means for the key concepts on the mushroom question.  What
# I think furthers the low IRR on the ABR NI is that EvoGrader has such a low mean -- 0.07 --
# that the scores of 0 from the humans are not informative as far as IRR, because you'd expect
# nearly all human 0s to be machine 0s as well even if they were assigned at random.

#